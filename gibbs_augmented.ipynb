{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyONHtE7bbxFspPUQ15j7lB+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fpgmina/gibbs/blob/main/gibbs_augmented.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import invgamma, gamma\n",
        "from typing import Tuple"
      ],
      "metadata": {
        "id": "dnlyitNDsuyU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rdfxBedisrtD"
      },
      "outputs": [],
      "source": [
        "def gibbs_sharpe_student(\n",
        "    returns: np.ndarray,\n",
        "    nu: float = 3.0,\n",
        "    n_iter: int = 5000,\n",
        "    burn_in: int = 500,\n",
        "    mu0: float = 0.0,\n",
        "    tau2: float = 1000.0,\n",
        "    a0: float = 2.0,\n",
        "    b0: float = 2.0,\n",
        "    seed: int = 42\n",
        ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Bayesian Sharpe ratio estimation using Gibbs sampling for t-distributed returns.\n",
        "\n",
        "    Uses data augmentation: the t-distribution is represented as a scale mixture of normals\n",
        "    with latent scaling variables lambda_i.\n",
        "\n",
        "    Model:\n",
        "        r_i | mu, sigma^2, lambda_i ~ N(mu, sigma^2 / lambda_i)\n",
        "        lambda_i ~ Gamma(nu/2, nu/2) independently\n",
        "        mu ~ N(mu0, tau2)\n",
        "        sigma^2 ~ Inverse-Gamma(a0, b0)\n",
        "        Sharpe ratio S = mu / sqrt(sigma^2)\n",
        "\n",
        "    Parameters:\n",
        "        returns (np.ndarray): 1D array of observed returns.\n",
        "        nu (float): Degrees of freedom for the t-distribution (typical fat-tailed: 2 < nu < 10).\n",
        "        n_iter (int): Number of Gibbs iterations.\n",
        "        burn_in (int): Number of burn-in samples to discard.\n",
        "        mu0 (float): Prior mean for mu (location parameter).\n",
        "        tau2 (float): Prior variance for mu.\n",
        "        a0 (float): Shape parameter for Inverse-Gamma prior on sigma^2.\n",
        "        b0 (float): Scale parameter for Inverse-Gamma prior on sigma^2.\n",
        "        seed (int): Random seed for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "            Arrays of posterior samples (after burn-in) for mu, sigma^2, and Sharpe ratio.\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    n = len(returns)\n",
        "\n",
        "    # Initializations\n",
        "    mu = np.mean(returns)\n",
        "    sigma2 = np.var(returns, ddof=1)\n",
        "    lambdas = np.ones(n)  # latent variables\n",
        "\n",
        "    mu_samples = np.zeros(n_iter)\n",
        "    sigma2_samples = np.zeros(n_iter)\n",
        "    sharpe_samples = np.zeros(n_iter)\n",
        "\n",
        "    for i in range(n_iter):\n",
        "        # 1. Update latent variables lambda_i | rest\n",
        "        resids = returns - mu\n",
        "        # Each lambda_i ~ Gamma((nu+1)/2, (nu + resid_i^2/sigma2)/2)\n",
        "        shape = (nu + 1) / 2.0\n",
        "        scale = 2.0 / (nu + (resids ** 2) / sigma2)  # gamma in scipy uses scale=1/rate\n",
        "        lambdas = gamma.rvs(a=shape, scale=scale)\n",
        "\n",
        "        # 2. Update mu | rest (Normal)\n",
        "        tau2_n = 1.0 / (1.0 / tau2 + np.sum(lambdas) / sigma2)\n",
        "        mu_n = tau2_n * (mu0 / tau2 + np.sum(lambdas * returns) / sigma2)\n",
        "        mu = np.random.normal(mu_n, np.sqrt(tau2_n))\n",
        "\n",
        "        # 3. Update sigma^2 | rest (Inverse-Gamma)\n",
        "        a_n = a0 + n / 2.0\n",
        "        b_n = b0 + 0.5 * np.sum(lambdas * (returns - mu) ** 2)\n",
        "        sigma2 = invgamma.rvs(a=a_n, scale=b_n)\n",
        "\n",
        "        # Store\n",
        "        mu_samples[i] = mu\n",
        "        sigma2_samples[i] = sigma2\n",
        "        sharpe_samples[i] = mu / np.sqrt(sigma2)\n",
        "\n",
        "    # Discard burn-in\n",
        "    return (\n",
        "        mu_samples[burn_in:],\n",
        "        sigma2_samples[burn_in:],\n",
        "        sharpe_samples[burn_in:]\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0bLaHjYCs4dX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}